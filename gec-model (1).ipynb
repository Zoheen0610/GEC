{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:12.794398Z","iopub.execute_input":"2025-03-30T18:00:12.794788Z","iopub.status.idle":"2025-03-30T18:00:12.799727Z","shell.execute_reply.started":"2025-03-30T18:00:12.794756Z","shell.execute_reply":"2025-03-30T18:00:12.798698Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install transformers datasets sacrebleu sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:15.173809Z","iopub.execute_input":"2025-03-30T18:00:15.174155Z","iopub.status.idle":"2025-03-30T18:00:19.895509Z","shell.execute_reply.started":"2025-03-30T18:00:15.174121Z","shell.execute_reply":"2025-03-30T18:00:19.894552Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.5.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\ndataset = load_dataset(\"juancavallotti/multilingual-gec\")\n\nprint(\"datasetdict looks like:\",dataset)\n\n# Keep only relevant columns for eng\ntrain_dataset = dataset[\"train\"].filter(lambda example: example[\"lang\"] == \"en\")\ntrain_dataset = train_dataset.remove_columns([\"transformation\", \"sec_transformation\", \"__index_level_0__\"])\n\nprint(train_dataset[0:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:19.896849Z","iopub.execute_input":"2025-03-30T18:00:19.897170Z","iopub.status.idle":"2025-03-30T18:00:24.165305Z","shell.execute_reply.started":"2025-03-30T18:00:19.897132Z","shell.execute_reply":"2025-03-30T18:00:24.164319Z"}},"outputs":[{"name":"stdout","text":"datasetdict looks like: DatasetDict({\n    train: Dataset({\n        features: ['lang', 'sentence', 'modified', 'transformation', 'sec_transformation', '__index_level_0__'],\n        num_rows: 216318\n    })\n    test: Dataset({\n        features: ['lang', 'sentence', 'modified', 'transformation', 'sec_transformation', '__index_level_0__'],\n        num_rows: 2186\n    })\n})\n{'lang': ['en', 'en', 'en', 'en', 'en'], 'sentence': ['Plants, obviously, cannot move after they have put down roots.', 'I looked at the schedule.', \"It's very hard to get rid of bad habits.\", \"Anyway, I think I've said enough.\", 'Technologies allow you to do more things.'], 'modified': [\"fix grammar: Plants, obviously, cannot moved after they hadn't put down roots.\", 'fix grammar: I looked at schedule.', 'fix grammar: It am very hard to get rid of bad habits.', \"fix grammar: Anyway, think I've said enough.\", 'fix grammar: Technologies allow you to do most things.']}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"processed_data = {\n    \"input_text\": train_dataset[\"modified\"],\n    \"target_text\": train_dataset[\"sentence\"]\n}\n\n\n# Convert to Hugging Face Dataset format\nformatted_dataset = Dataset.from_dict(processed_data)\n\n\nfor i in range(5):  \n    print(f\"Example {i+1}:\")\n    print(f\"  Input: {formatted_dataset['input_text'][i]}\")\n    print(f\"  Target: {formatted_dataset['target_text'][i]}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:25.702155Z","iopub.execute_input":"2025-03-30T18:00:25.702576Z","iopub.status.idle":"2025-03-30T18:00:26.716104Z","shell.execute_reply.started":"2025-03-30T18:00:25.702538Z","shell.execute_reply":"2025-03-30T18:00:26.715033Z"}},"outputs":[{"name":"stdout","text":"Example 1:\n  Input: fix grammar: Plants, obviously, cannot moved after they hadn't put down roots.\n  Target: Plants, obviously, cannot move after they have put down roots.\n--------------------------------------------------\nExample 2:\n  Input: fix grammar: I looked at schedule.\n  Target: I looked at the schedule.\n--------------------------------------------------\nExample 3:\n  Input: fix grammar: It am very hard to get rid of bad habits.\n  Target: It's very hard to get rid of bad habits.\n--------------------------------------------------\nExample 4:\n  Input: fix grammar: Anyway, think I've said enough.\n  Target: Anyway, I think I've said enough.\n--------------------------------------------------\nExample 5:\n  Input: fix grammar: Technologies allow you to do most things.\n  Target: Technologies allow you to do more things.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:30.018133Z","iopub.execute_input":"2025-03-30T18:00:30.018515Z","iopub.status.idle":"2025-03-30T18:00:30.030280Z","shell.execute_reply.started":"2025-03-30T18:00:30.018475Z","shell.execute_reply":"2025-03-30T18:00:30.029383Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# Tokenize input and target text\ndef tokenize_function(examples):\n    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128)\n    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n    \n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": targets[\"input_ids\"]\n    }\n\n\ntokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n\nprint(tokenized_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:33.211380Z","iopub.execute_input":"2025-03-30T18:00:33.211796Z","iopub.status.idle":"2025-03-30T18:00:49.502734Z","shell.execute_reply.started":"2025-03-30T18:00:33.211768Z","shell.execute_reply":"2025-03-30T18:00:49.501842Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50862 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129d1e4af0c343eaa47dca81b05b73e7"}},"metadata":{}},{"name":"stdout","text":"{'input_text': \"fix grammar: Plants, obviously, cannot moved after they hadn't put down roots.\", 'target_text': 'Plants, obviously, cannot move after they have put down roots.', 'input_ids': [2210, 19519, 10, 6041, 7, 6, 6865, 6, 1178, 2301, 227, 79, 12381, 31, 17, 474, 323, 8523, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [6041, 7, 6, 6865, 6, 1178, 888, 227, 79, 43, 474, 323, 8523, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_size = int(0.8 * len(tokenized_dataset))\ntrain_dataset = tokenized_dataset.select(range(train_size))\neval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:53.324369Z","iopub.execute_input":"2025-03-30T18:00:53.324793Z","iopub.status.idle":"2025-03-30T18:00:53.694781Z","shell.execute_reply.started":"2025-03-30T18:00:53.324757Z","shell.execute_reply":"2025-03-30T18:00:53.693991Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import  TrainerCallback\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/t5-grammar-corrector\",  \n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_steps=50, \n    save_steps=500,  \n    evaluation_strategy=\"steps\",  \n    eval_steps=500,  \n    save_total_limit=2, \n    fp16=True,\n    report_to=\"none\", \n)\n\n\nclass ProgressCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            print(f\"Step {state.global_step}: Loss = {logs.get('loss', 'N/A')}, LR = {logs.get('learning_rate', 'N/A')}\", flush=True)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    callbacks=[ProgressCallback()],\n)\n\n\ntrainer.train()\n\n\nmodel.save_pretrained(\"/kaggle/working/t5-grammar-corrector\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:55:07.390068Z","iopub.execute_input":"2025-03-30T13:55:07.390438Z","iopub.status.idle":"2025-03-30T16:22:58.079857Z","shell.execute_reply.started":"2025-03-30T13:55:07.390377Z","shell.execute_reply":"2025-03-30T16:22:58.078760Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7632' max='7632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7632/7632 2:27:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.027000</td>\n      <td>0.018151</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.017800</td>\n      <td>0.015229</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.017200</td>\n      <td>0.013195</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.015400</td>\n      <td>0.012304</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.014000</td>\n      <td>0.011244</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.012400</td>\n      <td>0.011037</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.011100</td>\n      <td>0.010668</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.010700</td>\n      <td>0.010229</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.009500</td>\n      <td>0.010147</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.011500</td>\n      <td>0.009619</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.008800</td>\n      <td>0.009802</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.009000</td>\n      <td>0.009601</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.008300</td>\n      <td>0.009528</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.008300</td>\n      <td>0.009530</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.008900</td>\n      <td>0.009490</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step 50: Loss = 3.093, LR = 4.9672431865828094e-05\nStep 100: Loss = 0.1017, LR = 4.9344863731656185e-05\nStep 150: Loss = 0.043, LR = 4.9017295597484283e-05\nStep 200: Loss = 0.0337, LR = 4.8689727463312375e-05\nStep 250: Loss = 0.0284, LR = 4.8362159329140466e-05\nStep 300: Loss = 0.0292, LR = 4.803459119496855e-05\nStep 350: Loss = 0.0258, LR = 4.770702306079665e-05\nStep 400: Loss = 0.0287, LR = 4.737945492662474e-05\nStep 450: Loss = 0.0257, LR = 4.705188679245283e-05\nStep 500: Loss = 0.027, LR = 4.672431865828092e-05\nStep 500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 550: Loss = 0.024, LR = 4.6396750524109015e-05\nStep 600: Loss = 0.0212, LR = 4.606918238993711e-05\nStep 650: Loss = 0.023, LR = 4.5741614255765204e-05\nStep 700: Loss = 0.0205, LR = 4.5414046121593296e-05\nStep 750: Loss = 0.0193, LR = 4.508647798742139e-05\nStep 800: Loss = 0.021, LR = 4.475890985324948e-05\nStep 850: Loss = 0.0179, LR = 4.443134171907757e-05\nStep 900: Loss = 0.0194, LR = 4.410377358490566e-05\nStep 950: Loss = 0.0185, LR = 4.377620545073375e-05\nStep 1000: Loss = 0.0178, LR = 4.3448637316561844e-05\nStep 1000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 1050: Loss = 0.019, LR = 4.312106918238994e-05\nStep 1100: Loss = 0.0185, LR = 4.2793501048218034e-05\nStep 1150: Loss = 0.0175, LR = 4.2465932914046125e-05\nStep 1200: Loss = 0.018, LR = 4.213836477987422e-05\nStep 1250: Loss = 0.0176, LR = 4.181079664570231e-05\nStep 1300: Loss = 0.0163, LR = 4.14832285115304e-05\nStep 1350: Loss = 0.0153, LR = 4.115566037735849e-05\nStep 1400: Loss = 0.0159, LR = 4.082809224318658e-05\nStep 1450: Loss = 0.0191, LR = 4.0500524109014674e-05\nStep 1500: Loss = 0.0172, LR = 4.017295597484277e-05\nStep 1500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 1550: Loss = 0.0164, LR = 3.984538784067086e-05\nStep 1600: Loss = 0.0144, LR = 3.9517819706498955e-05\nStep 1650: Loss = 0.015, LR = 3.9190251572327046e-05\nStep 1700: Loss = 0.0152, LR = 3.886268343815514e-05\nStep 1750: Loss = 0.0157, LR = 3.8535115303983236e-05\nStep 1800: Loss = 0.0144, LR = 3.820754716981133e-05\nStep 1850: Loss = 0.0159, LR = 3.787997903563941e-05\nStep 1900: Loss = 0.0154, LR = 3.75524109014675e-05\nStep 1950: Loss = 0.0163, LR = 3.7224842767295595e-05\nStep 2000: Loss = 0.0154, LR = 3.689727463312369e-05\nStep 2000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 2200: Loss = 0.0152, LR = 3.558700209643606e-05\nStep 2250: Loss = 0.0157, LR = 3.525943396226416e-05\nStep 2300: Loss = 0.0145, LR = 3.493186582809225e-05\nStep 2350: Loss = 0.0149, LR = 3.460429769392033e-05\nStep 2400: Loss = 0.0155, LR = 3.4276729559748424e-05\nStep 2450: Loss = 0.0152, LR = 3.394916142557652e-05\nStep 2500: Loss = 0.014, LR = 3.3621593291404614e-05\nStep 2500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 2550: Loss = 0.0149, LR = 3.3294025157232705e-05\nStep 2600: Loss = 0.0116, LR = 3.2966457023060796e-05\nStep 2650: Loss = 0.0115, LR = 3.263888888888889e-05\nStep 2700: Loss = 0.0129, LR = 3.2311320754716986e-05\nStep 2750: Loss = 0.0136, LR = 3.198375262054508e-05\nStep 2800: Loss = 0.0119, LR = 3.165618448637317e-05\nStep 2850: Loss = 0.0115, LR = 3.132861635220126e-05\nStep 2900: Loss = 0.0101, LR = 3.100104821802935e-05\nStep 2950: Loss = 0.0103, LR = 3.067348008385744e-05\nStep 3000: Loss = 0.0124, LR = 3.0345911949685535e-05\nStep 3000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 3050: Loss = 0.0121, LR = 3.0018343815513626e-05\nStep 3100: Loss = 0.0117, LR = 2.969077568134172e-05\nStep 3150: Loss = 0.0117, LR = 2.9363207547169812e-05\nStep 3200: Loss = 0.0118, LR = 2.9035639412997907e-05\nStep 3250: Loss = 0.0111, LR = 2.8708071278826e-05\nStep 3300: Loss = 0.0111, LR = 2.838050314465409e-05\nStep 3350: Loss = 0.0116, LR = 2.8052935010482185e-05\nStep 3400: Loss = 0.0124, LR = 2.7725366876310273e-05\nStep 3450: Loss = 0.0115, LR = 2.7397798742138364e-05\nStep 3500: Loss = 0.0111, LR = 2.7070230607966455e-05\nStep 3500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 3550: Loss = 0.0108, LR = 2.674266247379455e-05\nStep 3600: Loss = 0.0104, LR = 2.641509433962264e-05\nStep 3650: Loss = 0.0107, LR = 2.6087526205450736e-05\nStep 3700: Loss = 0.0113, LR = 2.5759958071278828e-05\nStep 3750: Loss = 0.0116, LR = 2.543238993710692e-05\nStep 3800: Loss = 0.0102, LR = 2.5104821802935014e-05\nStep 3850: Loss = 0.0098, LR = 2.4777253668763102e-05\nStep 3900: Loss = 0.0111, LR = 2.4449685534591197e-05\nStep 3950: Loss = 0.0111, LR = 2.412211740041929e-05\nStep 4000: Loss = 0.0107, LR = 2.3794549266247383e-05\nStep 4000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 4050: Loss = 0.0121, LR = 2.346698113207547e-05\nStep 4100: Loss = 0.009, LR = 2.3139412997903566e-05\nStep 4150: Loss = 0.0104, LR = 2.2811844863731657e-05\nStep 4200: Loss = 0.0108, LR = 2.248427672955975e-05\nStep 4250: Loss = 0.0105, LR = 2.2156708595387844e-05\nStep 4300: Loss = 0.0115, LR = 2.182914046121593e-05\nStep 4350: Loss = 0.0096, LR = 2.1501572327044026e-05\nStep 4400: Loss = 0.0096, LR = 2.1174004192872118e-05\nStep 4450: Loss = 0.0103, LR = 2.0846436058700213e-05\nStep 4500: Loss = 0.0095, LR = 2.0518867924528304e-05\nStep 4500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 4550: Loss = 0.0109, LR = 2.0191299790356395e-05\nStep 4600: Loss = 0.0102, LR = 1.9863731656184487e-05\nStep 4650: Loss = 0.0099, LR = 1.9536163522012578e-05\nStep 4700: Loss = 0.0111, LR = 1.9208595387840673e-05\nStep 4750: Loss = 0.0106, LR = 1.8881027253668765e-05\nStep 4800: Loss = 0.0094, LR = 1.8553459119496856e-05\nStep 4850: Loss = 0.0097, LR = 1.8225890985324947e-05\nStep 4900: Loss = 0.0089, LR = 1.789832285115304e-05\nStep 4950: Loss = 0.0112, LR = 1.7570754716981134e-05\nStep 5000: Loss = 0.0115, LR = 1.7243186582809225e-05\nStep 5000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 5050: Loss = 0.0094, LR = 1.691561844863732e-05\nStep 5100: Loss = 0.0089, LR = 1.6588050314465408e-05\nStep 5150: Loss = 0.0087, LR = 1.6260482180293503e-05\nStep 5200: Loss = 0.0088, LR = 1.5932914046121594e-05\nStep 5250: Loss = 0.0088, LR = 1.5605345911949685e-05\nStep 5300: Loss = 0.008, LR = 1.527777777777778e-05\nStep 5350: Loss = 0.009, LR = 1.495020964360587e-05\nStep 5400: Loss = 0.0087, LR = 1.4622641509433963e-05\nStep 5450: Loss = 0.0095, LR = 1.4295073375262054e-05\nStep 5500: Loss = 0.0088, LR = 1.3967505241090148e-05\nStep 5500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 5550: Loss = 0.0081, LR = 1.363993710691824e-05\nStep 5600: Loss = 0.0075, LR = 1.331236897274633e-05\nStep 5650: Loss = 0.0088, LR = 1.2984800838574423e-05\nStep 5700: Loss = 0.0091, LR = 1.2657232704402517e-05\nStep 5750: Loss = 0.0076, LR = 1.232966457023061e-05\nStep 5800: Loss = 0.0091, LR = 1.2002096436058701e-05\nStep 5850: Loss = 0.0084, LR = 1.1674528301886793e-05\nStep 5900: Loss = 0.0087, LR = 1.1346960167714884e-05\nStep 5950: Loss = 0.0086, LR = 1.1019392033542977e-05\nStep 6000: Loss = 0.009, LR = 1.069182389937107e-05\nStep 6000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 6050: Loss = 0.0078, LR = 1.0364255765199162e-05\nStep 6100: Loss = 0.0084, LR = 1.0036687631027255e-05\nStep 6150: Loss = 0.0071, LR = 9.709119496855348e-06\nStep 6200: Loss = 0.0084, LR = 9.38155136268344e-06\nStep 6250: Loss = 0.0088, LR = 9.05398322851153e-06\nStep 6300: Loss = 0.0085, LR = 8.726415094339622e-06\nStep 6350: Loss = 0.0089, LR = 8.398846960167715e-06\nStep 6400: Loss = 0.0093, LR = 8.071278825995808e-06\nStep 6450: Loss = 0.0088, LR = 7.7437106918239e-06\nStep 6500: Loss = 0.0083, LR = 7.416142557651992e-06\nStep 6500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 6550: Loss = 0.0074, LR = 7.088574423480083e-06\nStep 6600: Loss = 0.0082, LR = 6.761006289308176e-06\nStep 6650: Loss = 0.0077, LR = 6.433438155136269e-06\nStep 6700: Loss = 0.0076, LR = 6.105870020964361e-06\nStep 6750: Loss = 0.0087, LR = 5.778301886792453e-06\nStep 6800: Loss = 0.0082, LR = 5.4507337526205454e-06\nStep 6850: Loss = 0.008, LR = 5.123165618448638e-06\nStep 6900: Loss = 0.0087, LR = 4.79559748427673e-06\nStep 6950: Loss = 0.0076, LR = 4.468029350104822e-06\nStep 7000: Loss = 0.0083, LR = 4.1404612159329145e-06\nStep 7000: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 7050: Loss = 0.0082, LR = 3.8128930817610063e-06\nStep 7100: Loss = 0.0075, LR = 3.4853249475890986e-06\nStep 7150: Loss = 0.009, LR = 3.1577568134171913e-06\nStep 7200: Loss = 0.0084, LR = 2.830188679245283e-06\nStep 7250: Loss = 0.0075, LR = 2.5026205450733754e-06\nStep 7300: Loss = 0.008, LR = 2.1750524109014676e-06\nStep 7350: Loss = 0.0082, LR = 1.8474842767295599e-06\nStep 7400: Loss = 0.0078, LR = 1.519916142557652e-06\nStep 7450: Loss = 0.0072, LR = 1.1923480083857442e-06\nStep 7500: Loss = 0.0089, LR = 8.647798742138365e-07\nStep 7500: Loss = N/A, LR = N/A\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 7550: Loss = 0.0091, LR = 5.372117400419287e-07\nStep 7600: Loss = 0.0092, LR = 2.09643605870021e-07\nStep 7632: Loss = N/A, LR = N/A\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pip install sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:00:57.231175Z","iopub.execute_input":"2025-03-30T18:00:57.231562Z","iopub.status.idle":"2025-03-30T18:01:00.916284Z","shell.execute_reply.started":"2025-03-30T18:00:57.231528Z","shell.execute_reply":"2025-03-30T18:01:00.915151Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.5.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nmodel_path = \"/kaggle/working/t5-grammar-corrector\"\nmodel = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\nprint(\"Model loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:01:01.371157Z","iopub.execute_input":"2025-03-30T18:01:01.371558Z","iopub.status.idle":"2025-03-30T18:01:07.047356Z","shell.execute_reply.started":"2025-03-30T18:01:01.371524Z","shell.execute_reply":"2025-03-30T18:01:07.046392Z"}},"outputs":[{"name":"stdout","text":"Model loaded\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\n\nsample = eval_dataset[0]\ninput_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\ntrue_output = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n\n\nmodel.eval()\nwith torch.no_grad():\n    pred_ids = model.generate(torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model.device))\n    predicted_text = tokenizer.decode(pred_ids[0], skip_special_tokens=True)\n\nprint(\"Input Text:\", input_text)\nprint(\"Expected Output:\", true_output)\nprint(\"Model Prediction:\", predicted_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:01:12.408624Z","iopub.execute_input":"2025-03-30T18:01:12.408971Z","iopub.status.idle":"2025-03-30T18:01:13.668645Z","shell.execute_reply.started":"2025-03-30T18:01:12.408946Z","shell.execute_reply":"2025-03-30T18:01:13.667670Z"}},"outputs":[{"name":"stdout","text":"Input Text: fix grammar: Things is improving in Algeria.\nExpected Output: Things are improving in Algeria.\nModel Prediction: Things are improving in Algeria.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nimport torch\nfrom transformers import GenerationConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Load BLEU metric\nbleu = evaluate.load(\"sacrebleu\")\n\n# Define Generation Configuration\ngeneration_config = GenerationConfig(\n    max_length=128,\n    num_beams=5,\n    early_stopping=True\n)\n\n# ✅ Define compute_metrics BEFORE using it\ndef compute_metrics(eval_preds):\n    predictions, labels = eval_preds\n\n    predictions = np.argmax(predictions, axis=-1)\n\n    decoded_preds = [\n        tokenizer.decode(model.generate(torch.tensor(pred).unsqueeze(0), **generation_config.to_dict())[0],\n                         skip_special_tokens=True)\n        for pred in predictions\n    ]\n\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n\n    bleu_score = bleu.compute(predictions=decoded_preds, references=[[lbl] for lbl in decoded_labels])[\"score\"]\n\n    exact_matches = sum(1 for pred, gt in zip(decoded_preds, decoded_labels) if pred.strip().lower() == gt.strip().lower())\n    accuracy = (exact_matches / len(decoded_labels)) * 100\n\n    return {\"exact_match\": accuracy, \"bleu_score\": bleu_score}\n\n# Training Arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/t5-grammar-corrector\",\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_steps=50, \n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_total_limit=2,\n    fp16=False,  \n    no_cuda=True,  \n    report_to=\"none\",\n)\n\n# Trainer\ntrainer = Seq2SeqTrainer(\n    model=model.to(\"cpu\"),  \n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,  # No more \"not defined\" error\n    tokenizer=tokenizer  \n)\n\n# Run Evaluation\nresults = trainer.evaluate()\n\n# Print Scores\nprint(f\"Exact Match Accuracy: {results.get('eval_exact_match', 0):.2f}%\")\nprint(f\"BLEU Score: {results.get('eval_bleu_score', 0):.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:03:04.741398Z","iopub.execute_input":"2025-03-30T18:03:04.741836Z","execution_failed":"2025-03-30T18:13:34.289Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-41271611f2cb>:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 103/1272 10:11 < 1:56:44, 0.17 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null}]}